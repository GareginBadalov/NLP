{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ea501f6a",
      "metadata": {
        "id": "ea501f6a"
      },
      "source": [
        "# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –∫ —É—Ä–æ–∫—É 14. Transfer learning\n",
        "\n",
        "\n",
        "1. –í–∑—è—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑\n",
        "\n",
        "https://www.kaggle.com/datasets/mrapplexz/bashim-quotes\n",
        "\n",
        "–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–∏—Ö —Ü–∏—Ç–∞—Ç\n",
        "\n",
        "2. –í–∑—è—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑\n",
        "\n",
        "https://github.com/natasha/corus\n",
        "\n",
        "load_lenta2\n",
        "\n",
        "–Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —Å–∞–º —Ç–µ–∫—Å—Ç –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
        "\n",
        "–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å T5/ –∏–ª–∏ GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff82a98",
      "metadata": {
        "id": "eff82a98"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATASET_PATH = 'dataset.jsonl'\n",
        "\n",
        "\n",
        "with open(DATASET_PATH) as f: \n",
        "    df = pd.read_json(DATASET_PATH, lines=True).set_index('id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564d8e02",
      "metadata": {
        "id": "564d8e02",
        "outputId": "91acf9fe-07b3-4526-9718-a8f908ddaf1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>rating</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-08-30 11:24:00+00:00</td>\n",
              "      <td>22010.0</td>\n",
              "      <td>&lt;Ares&gt; ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-08-30 11:25:00+00:00</td>\n",
              "      <td>25105.0</td>\n",
              "      <td>&lt;—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥&gt; –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-08-30 11:27:00+00:00</td>\n",
              "      <td>7192.0</td>\n",
              "      <td>&lt;–î–æ—Ä&gt; \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-08-30 11:28:00+00:00</td>\n",
              "      <td>29169.0</td>\n",
              "      <td>&lt;PPDV[os2]&gt; \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2004-08-30 11:26:00+00:00</td>\n",
              "      <td>7140.0</td>\n",
              "      <td>&lt;Ohtori_Akio&gt; –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        date   rating  \\\n",
              "id                                      \n",
              "1  2004-08-30 11:24:00+00:00  22010.0   \n",
              "2  2004-08-30 11:25:00+00:00  25105.0   \n",
              "3  2004-08-30 11:27:00+00:00   7192.0   \n",
              "4  2004-08-30 11:28:00+00:00  29169.0   \n",
              "5  2004-08-30 11:26:00+00:00   7140.0   \n",
              "\n",
              "                                                 text  \n",
              "id                                                     \n",
              "1   <Ares> ppdv, –≤—Å–µ —é–Ω–∏–∫—Å—ã –æ—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã.. –æ–Ω–∏...  \n",
              "2   <—Ç–æ–º–∞—Ç–∏–∫_—Ä–∞–¥> –∞ —Ç—ã –Ω–µ —á—É–≤—Å—Ç–≤—É–µ—à—å –∫—Ä–∞—Å–æ—Ç—É –º–∏—Ä–∞?...  \n",
              "3   <–î–æ—Ä> \"–º—ã—à–∫–∞, –ø–æ—á–µ–º—É —É —Ç–µ–±—è —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –≥–ª–∞–∑...  \n",
              "4   <PPDV[os2]> \"–ú–∞–ª—å—á–∏–∫–∏, –≤—ã —á—Ç–æ –±–æ–ª—å–Ω—ã–µ, –±–µ–≥–∞—Ç—å ...  \n",
              "5   <Ohtori_Akio> –º—ã - –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ - –∂–∏–≤—ë–º —Å ...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c35aa8",
      "metadata": {
        "id": "14c35aa8",
        "outputId": "32fd2043-fab9-48b6-d4e9-e23b51b3e3dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(81497, 3)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcdbde5f",
      "metadata": {
        "id": "dcdbde5f"
      },
      "source": [
        "##### –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e7ac2b",
      "metadata": {
        "id": "91e7ac2b"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = df.loc[:5000, 'text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e96f21",
      "metadata": {
        "id": "62e96f21"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8b1eef",
      "metadata": {
        "id": "fe8b1eef"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def build_text_files(data_json, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    for texts in data_json:\n",
        "        summary = str(texts).strip()\n",
        "        summary = re.sub(r\"\\[\\w+\\]\", \"\", summary)\n",
        "        summary = re.sub(r\"<[\\w+,\\!, -]>\", \"\", summary)\n",
        "        summary = re.sub(r\"<\\w+>\", \"\", summary)\n",
        "        summary = re.sub(r\"\\s\", \" \", summary)\n",
        "        data += summary + \"  \"\n",
        "    f.write(data)\n",
        "  \n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d05497",
      "metadata": {
        "id": "60d05497"
      },
      "outputs": [],
      "source": [
        "with open('train_dataset.txt') as f: \n",
        "  train_dataset = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9ff8e1",
      "metadata": {
        "id": "9e9ff8e1"
      },
      "source": [
        "##### –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf21beee",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0489337b75aa436696348a3ee79b80a2",
            "898f019b16134fde9b5c04d443c2c028",
            "999f3fe38a8f4636b3e41f19cb23179f"
          ]
        },
        "id": "cf21beee",
        "outputId": "728c8090-f547-422f-9f32-9c98b441316b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0489337b75aa436696348a3ee79b80a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "898f019b16134fde9b5c04d443c2c028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "999f3fe38a8f4636b3e41f19cb23179f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer\n",
        "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e27f365",
      "metadata": {
        "id": "0e27f365"
      },
      "outputs": [],
      "source": [
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c041646",
      "metadata": {
        "id": "4c041646",
        "outputId": "13ec7670-d052-4bc0-a0c2-edfe1421b063"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alenakukhta/opt/anaconda3/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e398b7",
      "metadata": {
        "id": "59e398b7"
      },
      "source": [
        "##### –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45eabd15",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4dd3706dc0d84e67a23c4ee4051b3797"
          ]
        },
        "id": "45eabd15",
        "outputId": "f8cbf7a6-70df-4e78-ee44-5635e18f461c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dd3706dc0d84e67a23c4ee4051b3797",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/526M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f68e14a",
      "metadata": {
        "id": "0f68e14a"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gdrive/MyDrive/GPT/gpt2-chief\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d059730",
      "metadata": {
        "id": "6d059730"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e478840b",
      "metadata": {
        "id": "e478840b",
        "outputId": "20f5ac15-f53d-499b-ee2f-0ec3a0d27879"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alenakukhta/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 467\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 351\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [351/351 42:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=351, training_loss=4.383714637865029, metrics={'train_runtime': 2538.1042, 'train_samples_per_second': 0.552, 'train_steps_per_second': 0.138, 'total_flos': 91517534208000.0, 'train_loss': 4.383714637865029, 'epoch': 3.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2701819",
      "metadata": {
        "id": "b2701819"
      },
      "source": [
        "##### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319247f2",
      "metadata": {
        "id": "319247f2",
        "outputId": "8c0db898-c6f7-46d5-e34d-4169b12d7e78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to gdrive/MyDrive/GPT/gpt2-chief\n",
            "Configuration saved in gdrive/MyDrive/GPT/gpt2-chief/config.json\n",
            "Model weights saved in gdrive/MyDrive/GPT/gpt2-chief/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8781e681",
      "metadata": {
        "id": "8781e681",
        "outputId": "db762185-381c-4862-9ff5-d2969218dcab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in gdrive/MyDrive/GPT/gpt2-chief/tokenizer_config.json\n",
            "Special tokens file saved in gdrive/MyDrive/GPT/gpt2-chief/special_tokens_map.json\n",
            "Configuration saved in gdrive/MyDrive/GPT/model_gpt_chf/config.json\n",
            "Model weights saved in gdrive/MyDrive/GPT/model_gpt_chf/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "tokenizer.save_pretrained('gdrive/MyDrive/GPT/gpt2-chief')\n",
        "model.save_pretrained('gdrive/MyDrive/GPT/model_gpt_chf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d855ed",
      "metadata": {
        "id": "85d855ed"
      },
      "source": [
        "##### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7bd1c2b",
      "metadata": {
        "id": "a7bd1c2b",
        "outputId": "04f06819-fa4f-46d6-c0d4-9e099b2dba5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file gdrive/MyDrive/GPT/gpt2-chief/vocab.json\n",
            "loading file gdrive/MyDrive/GPT/gpt2-chief/merges.txt\n",
            "loading file gdrive/MyDrive/GPT/gpt2-chief/tokenizer.json\n",
            "loading file gdrive/MyDrive/GPT/gpt2-chief/added_tokens.json\n",
            "loading file gdrive/MyDrive/GPT/gpt2-chief/special_tokens_map.json\n",
            "loading file gdrive/MyDrive/GPT/gpt2-chief/tokenizer_config.json\n",
            "loading configuration file gdrive/MyDrive/GPT/model_gpt_chf/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gdrive/MyDrive/GPT/model_gpt_chf\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "loading weights file gdrive/MyDrive/GPT/model_gpt_chf/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gdrive/MyDrive/GPT/model_gpt_chf.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gdrive/MyDrive/GPT/gpt2-chief\")\n",
        "model1 = AutoModelForCausalLM.from_pretrained(\"gdrive/MyDrive/GPT/model_gpt_chf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f928da",
      "metadata": {
        "id": "f3f928da",
        "outputId": "9132c264-ae18-4d8b-9409-37723c5d6882"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å  <@dr-zan> —É –Ω–∞—Å –≤ –≥–æ—Ä–æ–¥–µ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Å–∫–∞–∑–∞—Ç—å —á—Ç–æ-—Ç–æ —Ç–∞–∫–æ–µ, —á–µ–≥–æ –Ω–µ —Å–∫–∞–∑–∞–ª –±—ã –Ω–∏ –æ–¥–∏–Ω –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —á–µ–ª–æ–≤–µ–∫.   –∞ –∫—Ç–æ –Ω–∏–±—É–¥—å –∑–Ω–∞–µ—Ç –≥–¥–µ –º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –¥—Ä–∞–π–≤–µ—Ä –Ω–∞ –≤–∏–Ω–¥—É\n"
          ]
        }
      ],
      "source": [
        "prefix = \"–¥–æ–±—Ä—ã–π –¥–µ–Ω—å \"\n",
        "tokens = tokenizer(prefix, return_tensors='pt')\n",
        "size = tokens['input_ids'].shape[1]\n",
        "output = model.generate( \n",
        "    **tokens, \n",
        "    do_sample=False,\n",
        "    max_length=+50,\n",
        "    repetition_penalty=5., \n",
        "    temperature=0.5,\n",
        "    num_beams=10,\n",
        ")\n",
        "\n",
        "decoded = tokenizer.decode(output[0])\n",
        "result = decoded[len(prefix):]\n",
        "print(prefix + result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265fcc91",
      "metadata": {
        "id": "265fcc91"
      },
      "source": [
        "#### 2. –í–∑—è—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑\n",
        "\n",
        "https://github.com/natasha/corus\n",
        "\n",
        "load_lenta2\n",
        "\n",
        "–Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —Å–∞–º —Ç–µ–∫—Å—Ç –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
        "\n",
        "–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å T5/ –∏–ª–∏ GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–µ–π"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4446ec",
      "metadata": {
        "id": "4a4446ec"
      },
      "source": [
        "##### –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a291fc",
      "metadata": {
        "id": "c9a291fc",
        "outputId": "3363626a-0005-4fbd-b911-6382e076091c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "--2022-07-18 12:18:08--  https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2014.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/141595206/7f9b5080-d24d-11e8-8b67-cd6a839c2b29?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220718%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220718T091809Z&X-Amz-Expires=300&X-Amz-Signature=dfb03e6818abc4846fac12924f95256fe8381989a9df753a56e2c5ce1f31caf4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141595206&response-content-disposition=attachment%3B%20filename%3Dnews-articles-2014.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-18 12:18:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/141595206/7f9b5080-d24d-11e8-8b67-cd6a839c2b29?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220718%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220718T091809Z&X-Amz-Expires=300&X-Amz-Signature=dfb03e6818abc4846fac12924f95256fe8381989a9df753a56e2c5ce1f31caf4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141595206&response-content-disposition=attachment%3B%20filename%3Dnews-articles-2014.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318705497 (304M) [application/octet-stream]\n",
            "Saving to: 'news-articles-2014.tar.bz2'\n",
            "\n",
            "news-articles-2014. 100%[===================>] 303.94M  15.0MB/s    in 15s     \n",
            "\n",
            "2022-07-18 12:18:25 (19.8 MB/s) - 'news-articles-2014.tar.bz2' saved [318705497/318705497]\n",
            "\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "--2022-07-18 12:18:25--  https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2015-part1.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/141595206/9d67b600-d24b-11e8-9f0f-5cb826a4ecec?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220718%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220718T091825Z&X-Amz-Expires=300&X-Amz-Signature=0ec8d3cf2c180b22583ff7790029b60c61e3c8a6ea296f1b99974515dc778c2d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141595206&response-content-disposition=attachment%3B%20filename%3Dnews-articles-2015-part1.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-18 12:18:25--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/141595206/9d67b600-d24b-11e8-9f0f-5cb826a4ecec?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220718%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220718T091825Z&X-Amz-Expires=300&X-Amz-Signature=0ec8d3cf2c180b22583ff7790029b60c61e3c8a6ea296f1b99974515dc778c2d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141595206&response-content-disposition=attachment%3B%20filename%3Dnews-articles-2015-part1.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 531645289 (507M) [application/octet-stream]\n",
            "Saving to: 'news-articles-2015-part1.tar.bz2'\n",
            "\n",
            "news-articles-2015- 100%[===================>] 507.02M  5.53MB/s    in 31s     \n",
            "\n",
            "2022-07-18 12:18:57 (16.2 MB/s) - 'news-articles-2015-part1.tar.bz2' saved [531645289/531645289]\n",
            "\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "--2022-07-18 12:18:57--  https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2015-part2.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/141595206/24695e00-d24d-11e8-9d0f-ab0a1d9ac829?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220718%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220718T091857Z&X-Amz-Expires=300&X-Amz-Signature=6269a9d5856f7830e6ca1c22ad2d501607a316756be3815e0447b6a3db3a75c6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141595206&response-content-disposition=attachment%3B%20filename%3Dnews-articles-2015-part2.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-18 12:18:57--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/141595206/24695e00-d24d-11e8-9d0f-ab0a1d9ac829?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220718%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220718T091857Z&X-Amz-Expires=300&X-Amz-Signature=6269a9d5856f7830e6ca1c22ad2d501607a316756be3815e0447b6a3db3a75c6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141595206&response-content-disposition=attachment%3B%20filename%3Dnews-articles-2015-part2.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 445562164 (425M) [application/octet-stream]\n",
            "Saving to: 'news-articles-2015-part2.tar.bz2'\n",
            "\n",
            "news-articles-2015- 100%[===================>] 424.92M  25.4MB/s    in 17s     \n",
            "\n",
            "2022-07-18 12:19:14 (24.8 MB/s) - 'news-articles-2015-part2.tar.bz2' saved [445562164/445562164]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2014.tar.bz2\n",
        "!wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2015-part1.tar.bz2\n",
        "!wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2015-part2.tar.bz2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1736cce",
      "metadata": {
        "id": "c1736cce",
        "outputId": "7fa0fe9c-1c67-440e-bc69-962e52757216"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BuriyRecord(\n",
              "    timestamp=datetime.datetime(2014, 8, 22, 15, 15),\n",
              "    url='http://www.ntv.ru/novosti/1200239/',\n",
              "    edition=None,\n",
              "    topics='novosti',\n",
              "    title='–†–æ—Å—Å–∏—è–Ω–µ –Ω–∞ —é–Ω–æ—à–µ—Å–∫–∏—Ö –û–ª–∏–º–ø–∏–π—Å–∫–∏—Ö –∏–≥—Ä–∞—Ö –≤ –ø—è—Ç–Ω–∏—Ü—É –∑–∞–≤–æ–µ–≤–∞–ª–∏ –≤–æ—Å–µ–º—å –º–µ–¥–∞–ª–µ–π',\n",
              "    text='–í –ø—è—Ç–Ω–∏—Ü—É —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω—ã –Ω–∞\\n—é–Ω–æ—à–µ—Å–∫–∏—Ö –û–ª–∏–º–ø–∏–π—Å–∫–∏—Ö –∏–≥—Ä–∞—Ö –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º –ù–∞–Ω–∫–∏–Ω–µ\\n–∑–∞–≤–æ–µ–≤–∞–ª–∏ –µ—â–µ –≤–æ—Å–µ–º—å –Ω–∞–≥—Ä–∞–¥ \\x97 –¥–≤–µ –∑–æ–ª–æ—Ç—ã–µ, —Ç—Ä–∏ —Å–µ—Ä–µ–±—Ä—è–Ω—ã–µ –∏ —Ç—Ä–∏ –±—Ä–æ–Ω–∑–æ–≤—ã–µ.\\n–¢—è–∂–µ–ª–æ–∞—Ç–ª–µ—Ç –•–µ—Ç–∞–≥ –•—É–≥–∞–µ–≤ –∑–∞–≤–æ–µ–≤–∞–ª –∑–æ–ª–æ—Ç–æ –≤ –≤–µ—Å–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ 85 –∫–≥. –†–æ–∑–∞–ª–∏—è –ù–∞—Å—Ä–µ—Ç–¥–∏–Ω–æ–≤–∞ –ø–µ—Ä–≤–µ–Ω—Å—Ç–≤–æ–≤–∞–ª–∞ –≤ –ø–ª–∞–≤–∞–Ω–∏–∏ –Ω–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ 50 –º–µ—Ç—Ä–æ–≤ –≤–æ–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º.\\n–ï–≤–≥–µ–Ω–∏–π –†—ã–ª–æ–≤ –ø—Ä–∏–Ω–µ—Å —Ä–æ—Å—Å–∏—è–Ω–∞–º —Å–µ—Ä–µ–±—Ä–æ –≤ –ø–ª–∞–≤–∞–Ω–∏–∏ –Ω–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ 200 –º–µ—Ç—Ä–æ–≤ –Ω–∞ —Å–ø–∏–Ω–µ. –û–Ω –∂–µ –≤–º–µ—Å—Ç–µ —Å –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–º –°–∞–¥–æ–≤–Ω–∏–∫–æ–≤—ã–º, –†–æ–∑–∞–ª–∏–µ–π –ù–∞—Å—Ä–µ—Ç–¥–∏–Ω–æ–≤–æ–π –∏ –î–∞—Ä—å–µ–π –£—Å—Ç–∏–Ω–æ–≤–æ–π —Å—Ç–∞–ª –≤—Ç–æ—Ä—ã–º –≤ —Å–º–µ—à–∞–Ω–Ω–æ–π –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —ç—Å—Ç–∞—Ñ–µ—Ç–µ 4—Ö100 –º.\\n–®—Ç–∞–Ω–≥–∏—Å—Ç–∫–∞ –°–≤–µ—Ç–ª–∞–Ω–∞ –©–µ—Ä–±–∞–∫–æ–≤–∞ –∑–∞–≤–æ–µ–≤–∞–ª–∞ —Å–µ—Ä–µ–±—Ä–æ –≤ –≤–µ—Å–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–≤—ã—à–µ 63 –∫–≥.\\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ç–∞ –∂–µ –î–∞—Ä—å—è –£—Å—Ç–∏–Ω–æ–≤–∞ –∑–∞–≤–æ–µ–≤–∞–ª–∞ –±—Ä–æ–Ω–∑—É –≤ –ø–ª–∞–≤–∞–Ω–∏–∏ –Ω–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ 50 –º –≤–æ–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º, –∞ –ê–Ω—Ç–æ–Ω –ß—É–ø–∫–æ–≤ \\x97 –Ω–∞–≥—Ä–∞–¥—É —Ç–æ–≥–æ –∂–µ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞ –Ω–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ 50 –º –Ω–∞ —Å–ø–∏–Ω–µ.\\n–ü–æ –∏—Ç–æ–≥–∞–º —à–µ—Å—Ç–æ–≥–æ –¥–Ω—è —Å–±–æ—Ä–Ω–∞—è –†–æ—Å—Å–∏–∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞ –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ –≤ –∫–æ–º–∞–Ω–¥–Ω–æ–º –∑–∞—á–µ—Ç–µ. –ù–∞ –µ–µ —Å—á–µ—Ç—É 29 –º–µ–¥–∞–ª–µ–π (11 –∑–æ–ª–æ—Ç—ã—Ö, 10 —Å–µ—Ä–µ–±—Ä—è–Ω—ã—Ö –∏ 8 –±—Ä–æ–Ω–∑–æ–≤—ã—Ö). –õ–∏–¥–∏—Ä—É–µ—Ç –∫–∏—Ç–∞–π—Å–∫–∞—è –∫–æ–º–∞–Ω–¥–∞ —Å 35 –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (17 \\x97 7 \\x97 11).'\n",
              ")"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from corus import load_buriy_news\n",
        "\n",
        "paths = [\n",
        "    'news-articles-2014.tar.bz2',\n",
        "    'news-articles-2015-part1.tar.bz2',\n",
        "    'news-articles-2015-part2.tar.bz2'\n",
        "]\n",
        "records = (\n",
        "    record\n",
        "    for path in paths\n",
        "    for record in load_buriy_news(path)\n",
        ")\n",
        "next(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb04b60f",
      "metadata": {
        "id": "cb04b60f"
      },
      "outputs": [],
      "source": [
        "data = [(record.title, record.text) for record in records]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de6e9e1",
      "metadata": {
        "id": "1de6e9e1",
        "outputId": "f3923d19-6a7d-44c7-8bc8-b640e6540643"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>–†–æ—Å—Å–∏–π—Å–∫–∏–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã –ø–æ–ø–∞–ª–∏ –ø–æ–¥ –º–∏–Ω–æ–º–µ—Ç–Ω—ã–π –æ–±...</td>\n",
              "      <td>–ü—ë—Ç—Ä –ú–∏—Ö–∞–π–ª–æ–≤\\n, —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø—Ä–æ–≤–æ–∑...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>–ï–° –ø—Ä–∏–∑—ã–≤–∞–µ—Ç –†–§ –≤—ã–≤–µ—Å—Ç–∏ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π –∫–æ–Ω–≤–æ–π —Å ...</td>\n",
              "      <td>–°–µ–±–∞—Å—Ç—å–µ–Ω –ë—Ä–∞–±–∞–Ω—Ç\\n, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ï–°: ¬´–ú—ã —Å–æ–∂...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>–Ø—Ü–µ–Ω—é–∫: —É –Ω–∞—Å –µ—Å—Ç—å —Å–≤–æ—è –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω–∞—è –ø–æ–º–æ—â—å</td>\n",
              "      <td>–ê—Ä—Å–µ–Ω–∏–π –Ø—Ü–µ–Ω—é–∫ –∑–∞—è–≤–∏–ª, —á—Ç–æ –£–∫—Ä–∞–∏–Ω–∞ –Ω–µ –Ω—É–∂–¥–∞–µ—Ç—Å...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>–ú—É–∑–µ–π –≤–∞—Ä–µ–∂–∫–∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤—ã—Å—Ç–∞–≤–∫—É —Ö—É–¥–æ–∂–Ω–∏–∫–∞-–∏–Ω–≤...</td>\n",
              "      <td>–°–µ–≥–æ–¥–Ω—è –≤ –ø–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–æ–º –º—É–∑–µ–µ –≤–∞—Ä–µ–∂–∫–∏ –æ—Ç–∫—Ä—ã–ª–∞—Å...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>–¢–∏—Ö–æ–Ω–æ–≤ —Å—Ç–∞–Ω–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º –æ—Ç –†–æ—Å—Å...</td>\n",
              "      <td>–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –¢–∏—Ö–æ–Ω–æ–≤ (–Ω–∞ —Ñ–æ—Ç–æ) –±—É–¥–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã –ø–æ–ø–∞–ª–∏ –ø–æ–¥ –º–∏–Ω–æ–º–µ—Ç–Ω—ã–π –æ–±...   \n",
              "1  –ï–° –ø—Ä–∏–∑—ã–≤–∞–µ—Ç –†–§ –≤—ã–≤–µ—Å—Ç–∏ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π –∫–æ–Ω–≤–æ–π —Å ...   \n",
              "2        –Ø—Ü–µ–Ω—é–∫: —É –Ω–∞—Å –µ—Å—Ç—å —Å–≤–æ—è –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω–∞—è –ø–æ–º–æ—â—å   \n",
              "3  –ú—É–∑–µ–π –≤–∞—Ä–µ–∂–∫–∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤—ã—Å—Ç–∞–≤–∫—É —Ö—É–¥–æ–∂–Ω–∏–∫–∞-–∏–Ω–≤...   \n",
              "4  –¢–∏—Ö–æ–Ω–æ–≤ —Å—Ç–∞–Ω–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º –æ—Ç –†–æ—Å—Å...   \n",
              "\n",
              "                                                text  \n",
              "0  –ü—ë—Ç—Ä –ú–∏—Ö–∞–π–ª–æ–≤\\n, —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø—Ä–æ–≤–æ–∑...  \n",
              "1  –°–µ–±–∞—Å—Ç—å–µ–Ω –ë—Ä–∞–±–∞–Ω—Ç\\n, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ï–°: ¬´–ú—ã —Å–æ–∂...  \n",
              "2  –ê—Ä—Å–µ–Ω–∏–π –Ø—Ü–µ–Ω—é–∫ –∑–∞—è–≤–∏–ª, —á—Ç–æ –£–∫—Ä–∞–∏–Ω–∞ –Ω–µ –Ω—É–∂–¥–∞–µ—Ç—Å...  \n",
              "3  –°–µ–≥–æ–¥–Ω—è –≤ –ø–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–æ–º –º—É–∑–µ–µ –≤–∞—Ä–µ–∂–∫–∏ –æ—Ç–∫—Ä—ã–ª–∞—Å...  \n",
              "4  –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –¢–∏—Ö–æ–Ω–æ–≤ (–Ω–∞ —Ñ–æ—Ç–æ) –±—É–¥–µ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º...  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_news = pd.DataFrame({'title': [record[0] for record in data], 'text': [record[1] for record in data]})\n",
        "df_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d614fc29",
      "metadata": {
        "id": "d614fc29",
        "outputId": "45199374-8b0f-4fc9-816c-6fb36eb6133b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2154800, 2)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_news.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ebd7296",
      "metadata": {
        "id": "1ebd7296",
        "outputId": "4c4e4dbe-c978-49a9-c9cf-49206e53d0fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(717050, 252)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(len(i) for i in df_news['text'].values), max(len(i) for i in df_news['title'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21bf353a",
      "metadata": {
        "id": "21bf353a"
      },
      "outputs": [],
      "source": [
        "data_news = df_news[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2424b37a",
      "metadata": {
        "id": "2424b37a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(data_news, test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddcc2827",
      "metadata": {
        "id": "ddcc2827",
        "outputId": "17153bcd-60a2-4e1c-f582-aecc0aefce5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['title', 'text'],\n",
              "     num_rows: 425\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['title', 'text'],\n",
              "     num_rows: 75\n",
              " }))"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_test = Dataset.from_pandas(df_test)\n",
        "dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc1ad80",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9c926eb2e3b3436c94c6443fb92a5f21",
            "de9d0379cef5480e8d518019d7a3059d"
          ]
        },
        "id": "7dc1ad80",
        "outputId": "c8c009b8-69f7-4731-c504-c8fe3e7058b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/spiece.model from cache at /Users/alenakukhta/.cache/huggingface/transformers/9adebe2aa47a25febcf707ee15540f9e440f8d8e79697dae237fc4e6ccad5019.b846524fbcbf3cf81e2302f8087043922ca4c445b4016bf16e707f7e2240a3e6\n",
            "loading file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/tokenizer.json from cache at /Users/alenakukhta/.cache/huggingface/transformers/9ea40ad78f35f554a9c607b947515d4e6f2f62e3a6a1ec3fa957e727cddbb635.975331e2a6fea3fd5d8f528410d471fb0f6f16b82a34e658f7a0d5eda5061b99\n",
            "loading file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/special_tokens_map.json from cache at /Users/alenakukhta/.cache/huggingface/transformers/6ef3ae62c46b2a0173c15263e19f4443143979a9eefdfc1feab6a709496e7be2.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\n",
            "loading file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/tokenizer_config.json from cache at /Users/alenakukhta/.cache/huggingface/transformers/f2a7b6aa9f667eabdf0660f437c38ca43696ef49a86df2e264fc93750ab9abb5.30a9c79b7a80d5e1aa9c5b7d6aff517e04661218d78a71459b4587fa83baa33a\n",
            "Parameter 'function'=<function tokenize at 0x7fbdbff3f3a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c926eb2e3b3436c94c6443fb92a5f21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de9d0379cef5480e8d518019d7a3059d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#—Å–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "max_len_txt = 400\n",
        "max_len_tlt = 50\n",
        "\n",
        "def tokenize(batch):\n",
        "    tokenized_input = tokenizer(batch['text'], padding='max_length', truncation=True, max_length=max_len_txt)\n",
        "    tokenized_label = tokenizer(batch['title'], padding='max_length', truncation=True, max_length=max_len_tlt)\n",
        "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
        "\n",
        "    return tokenized_input\n",
        "\n",
        "dataset_train = dataset_train.map(tokenize, batched=True, batch_size=8)\n",
        "dataset_test = dataset_test.map(tokenize, batched=True, batch_size=8)\n",
        "\n",
        "dataset_train.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dataset_test.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee04c54",
      "metadata": {
        "id": "4ee04c54",
        "outputId": "b49645e5-e4f8-4e62-fed4-62ab67cb1c05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "     num_rows: 425\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "     num_rows: 75\n",
              " }))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f3f4bd",
      "metadata": {
        "id": "b5f3f4bd"
      },
      "outputs": [],
      "source": [
        "dataset_train.save_to_disk('gazeta/train')\n",
        "dataset_test.save_to_disk('gazeta/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f5b9304",
      "metadata": {
        "id": "1f5b9304"
      },
      "source": [
        "##### –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5782dd32",
      "metadata": {
        "id": "5782dd32",
        "outputId": "c6fd4a24-117a-4089-8faf-4ad73beaafa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/config.json from cache at /Users/alenakukhta/.cache/huggingface/transformers/426a325da473aa010e527ee99032b35ce9354913e38282d34e50dd75856c82f7.87df939950b4282d4195b92cd0a6209ec6d3d69e74b13a77d87890cf3a5ded7b\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"cointegrated/rut5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"bos_token_id\": 2,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_length\": 200,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta/resolve/main/pytorch_model.bin from cache at /Users/alenakukhta/.cache/huggingface/transformers/f4c11e29521f27cff5768a7f18580488bf450ce29ef1e4c8e06f9802e9d6ab42.e3b906d7892e5f268c9d2f19a84d52ddfe543929f0dca80cb66a59459d424a1b\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at IlyaGusev/rut5_base_sum_gazeta.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "model_name = \"IlyaGusev/rut5_base_sum_gazeta\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6b46e4",
      "metadata": {
        "id": "1f6b46e4",
        "outputId": "6b8bac3a-5f8f-44e8-e8eb-358549b49760"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "#–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
        "output_dir = 'gazeta/output'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_steps=1000,\n",
        "    remove_unused_columns=True, \n",
        "    eval_steps=500,\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d3b2c77",
      "metadata": {
        "id": "4d3b2c77",
        "outputId": "44926ab2-85a3-454a-bf3e-523f5ff946fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "     num_rows: 425\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
              "     num_rows: 75\n",
              " }))"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "dataset_train = load_from_disk(\"gazeta/train\")\n",
        "dataset_test = load_from_disk(\"gazeta/test\")\n",
        "dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfb2608",
      "metadata": {
        "id": "cdfb2608",
        "outputId": "6c06d6be-f3e9-490c-c1c3-5b049b0d6486"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: text, title. If text, title are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "/Users/alenakukhta/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 425\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 639\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='639' max='639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [639/639 1:43:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.286200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=639, training_loss=2.789409130018828, metrics={'train_runtime': 6190.1052, 'train_samples_per_second': 0.206, 'train_steps_per_second': 0.103, 'total_flos': 677081548800000.0, 'train_loss': 2.789409130018828, 'epoch': 3.0})"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_test\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6288e314",
      "metadata": {
        "id": "6288e314",
        "outputId": "c1004fe3-5a55-497c-9f5a-587ab9ca07bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to gazeta/output/model\n",
            "Configuration saved in gazeta/output/model/config.json\n",
            "Model weights saved in gazeta/output/model/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model(output_dir + '/model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b0d1bd",
      "metadata": {
        "id": "c1b0d1bd"
      },
      "source": [
        "##### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å—Ç–∞—Ç–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93be91b",
      "metadata": {
        "id": "f93be91b",
        "outputId": "296458c4-46e3-4c85-eceb-5916076ca903"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text:\n",
            "–ò–∑ –∑–∞—è–≤–ª–µ–Ω–∏—è –ú–ò–î –†–§\n",
            ": ¬´21 –∞–≤–≥—É—Å—Ç–∞ –†–æ—Å—Å–∏—è –≤—ã—Å—Ç—É–ø–∏–ª–∞ —Å –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–æ–π –ø—Ä–∏–Ω—è—Ç–∏—è –°–æ–≤–µ—Ç–æ–º –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –û–û–ù —Ä–µ—à–µ–Ω–∏—è –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–æ—Å—Ç–∞–≤–∫–∏ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω–æ–π –ø–æ–º–æ—â–∏ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —é–≥–æ-–≤–æ—Å—Ç–æ–∫. –≠—Ç–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞ –±—ã–ª–∞ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –°–®–ê –∏ –õ–∏—Ç–≤–æ–π¬ª.\n",
            "–í –ú–ò–î –†–§ —Å—á–∏—Ç–∞—é—Ç –æ—á–µ–≤–∏–¥–Ω–æ–π –¥–≤—É–ª–∏—á–∏–µ —Ç–∞–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –æ–±–≤–∏–Ω—è—é—Ç –°–®–ê –∏ –∏—Ö –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏—Ö –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ –≤ ¬´—Ü–∏–Ω–∏—á–Ω–æ–º –ø—Ä–µ–Ω–µ–±—Ä–µ–∂–µ–Ω–∏–∏ —Å—É–¥—å–±–∞–º–∏ –º–∏—Ä–Ω—ã—Ö –≥—Ä–∞–∂–¥–∞–Ω –∏ –Ω–∞–ø–ª–µ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –∫ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–º—É –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω–æ–º—É –ø—Ä–∞–≤—É¬ª.\n",
            "–ò–∑ –∑–∞—è–≤–ª–µ–Ω–∏—è –ú–ò–î –†–§\n",
            ": ¬´–ï—Å–ª–∏ –°–®–ê –ø–æ—à–ª–∏ –ø—Ä–æ—Ç–∏–≤ –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–µ–∫–æ–Ω—Ñ—Ä–æ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ, –ø—Ä–∏–º–∏—Ä—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Ç–æ –Ω–µ –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∏–∫–∞–∫–∏—Ö —Å–æ–º–Ω–µ–Ω–∏–π –≤ –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –í–∞—à–∏–Ω–≥—Ç–æ–Ω–∞ –Ω–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤–æ–æ—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ. –ò–Ω–∞—á–µ –∫–∞–∫ –ø–æ–ø—ã—Ç–∫–æ–π ‚Äû–ø–æ–¥–æ—Ä–≤–∞—Ç—å‚Äú –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—É—é –º–∏—Å—Å–∏—é –Ω–∞–∑–≤–∞—Ç—å —ç—Ç–æ –Ω–µ–ª—å–∑—è¬ª.\n",
            "Real title: –ú–ò–î –†–§: –°–®–ê –ø—ã—Ç–∞—é—Ç—Å—è —Å–æ—Ä–≤–∞—Ç—å —Ä–æ—Å—Å–∏–π—Å–∫—É—é –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—É—é –º–∏—Å—Å–∏—é –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ\n",
            "Pred title: –ú–ò–î –†–§: –°–®–ê –∏ –õ–∏—Ç–≤–∞ –æ—Ç–∫–∞–∑–∞–ª–∏—Å—å –æ—Ç –¥–æ—Å—Ç–∞–≤–∫–∏ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω–æ–π –ø–æ–º–æ—â–∏ –Ω–∞ –£–∫—Ä–∞–∏–Ω–µ\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "INX = 10\n",
        "input_text = dataset_test['text'][INX]\n",
        "input_title = dataset_test['title'][INX]\n",
        "\n",
        "with torch.no_grad():\n",
        "    tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    source_ids = tokenized_text['input_ids'].to(dtype = torch.long)\n",
        "    source_mask = tokenized_text['attention_mask'].to(dtype = torch.long)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        input_ids = source_ids,\n",
        "        attention_mask = source_mask, \n",
        "        max_length=512,\n",
        "        num_beams=7,\n",
        "        temperature = 1.3,\n",
        "        repetition_penalty=1, \n",
        "        length_penalty=1, \n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(\"Text:\\n\" + input_text)\n",
        "print(\"Real title: \" + input_title)\n",
        "print(\"Pred title: \" + pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}